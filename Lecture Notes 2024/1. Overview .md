# Overview of Large Language Models
**Large Language Model Course**

**LLM Large Language Model Course Lecture Materials**
© 2024 by Matsuo-Iwasawa Lab, The University of Tokyo
2024
2024/09/04

---

## Speaker Profile

**Matsuo-Iwasawa Lab, The University of Tokyo**
**Yusuke Iwasawa**

Completed Ph.D. at the Graduate School of Engineering, The University of Tokyo in 2017 (Matsuo Lab). After working as a Project Researcher and Project Assistant Professor, became an Associate Professor in the Department of Technology Management for Innovation in January 2024.

**Research Themes, etc.**
-   **Until Master's:** Application of machine learning technology to support people with disabilities.
-   **From Ph.D.:** Research primarily on transfer learning techniques in Deep Learning.

### Activities
*   **DL Reading Circle:**
    Organizer of a study group participated in by Matsuo Lab members and lecture students. Held cumulatively over 350 times since 2015 (every Friday morning at 10:00).
*   **DL Book (Supervising Translator/Translator):**
    Supervising translator and translator of the "Deep Learning" textbook written by Goodfellow et al., published in 2018.
*   **Activities related to Generative AI:**
    -   "Large-Language Models are Zero-Shot Reasoners", NeurIPS2022, etc.
    -   Tutorials on "Technology and Prospects of Foundation Models" at JSAI2023 and CSS2023 [Speaker Deck].
    -   Overall design of the Matsuo Lab Large Language Model Course / Lecturer for Day 1 & Day 2.
    -   Lecture on Large Language Models to Prime Minister Kishida and others (180 minutes).

---

## Table of Contents

*   **Overview of LLMs** (Why learn LLMs?)
*   **Overview of each session**
*   **The environment surrounding LLMs in Japan**

---

## Motivation

*   Suppose you want to create an assistant AI that handles natural language right now.
*   For example, you want it to output the correct answer to a question.
    *   *Example:* Q. What is the capital of Japan? A. Tokyo.
*   For example, if you say "Translate this text into English," you want it to output the translated text.
*   For example, if you say "Tell me examples suitable for introducing language models," you want it to present several examples.

# What are Language Models?

*   When we have a sequence of words (sentence) $(x_1, x_2, \dots, x_L)$, the probabilistic model $p$ that assigns a **generation probability $p(x_1, x_2, \dots, x_L)$** to it.
    *   $p(\text{Japan, 's, capital, is, Tokyo}) = 0.02$
    *   $p(\text{Japan, 's, capital, is, Paris}) = 0.00001$
    *   $p(\text{Tokyo, 's, capital, is, Japan}) = 0.0005$
*   Various language tasks can be treated as problems of estimating this generation probability.
    *   Example: QA (What is the appropriate answer following a certain question?)
    *   Example: Translation (What is the natural Japanese following a certain English sentence?)
*   "How to determine the generation probability?" is the technical problem of language modeling.

---

# Autoregressive Language Models

*   Express $p(x_1, x_2, \dots, x_L)$ as a product of conditional distributions:
    $$p(x_1, x_2, \dots, x_L) = p(x_1)p(x_2|x_1) \dots p(x_L|x_1, x_2, \dots, x_{L-1})$$
*   Models that decompose probabilities into a chain like this are called **Autoregressive Language Models**.
*   If the conditional probability is known, generation is possible.
    *   $p(\text{Tokyo} | \text{Japan, 's, capital, is}) = 0.2$
    *   $p(\text{Paris} | \text{Japan, 's, capital, is}) = 0.001$
    *   ...
    *   $p(\text{Cairo} | \text{Japan, 's, capital, is}) = 0.0005$
    *   Result: Japan's capital is $\rightarrow$ **Tokyo**
*   How do we determine this conditional probability?

---

# Neural Language Models

*   Models that estimate the conditional probability using some form of Neural Network.
*   Similar to other learning methods, trained to maximize likelihood (Backpropagation).

*(Diagram shows input "Japan's", "capital", "is" passing through embeddings and hidden layers to output "Tokyo".)*

---

# Challenges of Neural Language Models

*   **Convolutional Networks (CNN) or MLPs** have difficulty processing long contexts.
    *   For example, in translation, it is necessary to determine the translated word by reflecting on the original sentence properly.
    *   There are tasks that cannot be solved without processing long-sequence information to some extent.

*   **RNN-based models** cannot learn in parallel, making scaling difficult.
    *   Due to the property of processing data sequentially, parallelization of learning is difficult.
    *   There is also the problem that learning itself is difficult (Vanishing Gradient Problem).

---

# "Attention is All You Need", NeurIPS 2017
## Transformer

*(Diagrams of Scaled Dot-Product Attention and Multi-Head Attention)*

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

*   Published in 2017 by a research team centered at Google.
*   A network structure centered on **Self-Attention**.
    *   *The detailed structure will be discussed on Day 3.*
*   Enables **parallelization of learning and inference** (Right figure: performance comparison).
    *   Example: English $\rightarrow$ Transformer $\rightarrow$ German
    *   Trained via backpropagation to match the target.

---

# "Improving Language Understanding by Generative Pre-training", 2018
## Generative Pretraining Transformer (GPT)

**Pre-training (Prior Learning)**
*   Input: Language models determine [mask]
*   Output: **word probability** by analyzing text data

*   Model published by OpenAI in 2018.
*   Uses the Transformer for **Pre-training** (Language model using Transformer).
*   Specifically, learns to predict the next word using Transformer (Left figure).
    *   Used the *Book Corpus* as training data.
*   With GPT, GPT-2, and GPT-3 versions, the amount of learning data and model size increased.

---

# Progress of Large Language Models

*(Chart: Timeline of LLMs including GPT-3, PaLM, Llama, etc.)*
*"A Survey of Large Language Models", 2023 (version 13)*

*(Table: Chatbot Arena rankings showing GPT-4, Claude 3, etc.)*

*   **GPT-4** (Rumored to be 1.8T parameters) and Gemini have been released as models beginning in 2023.
*   In 2024, models like Anthropic's **Claude-3**, Meta's **Llama-3** (Open Model), and Cohere's **Command R+** have emerged.

---

# Domain-Specific LLMs

*(Logos/Images: FinGPT, Med-PaLM M, Harvey)*

*   In addition to models created for general purposes like on the previous page, models trained to possess knowledge of **specific domains or skills** are also emerging.
    *   **Finance:** FinGPT
    *   **Medical:** Med-PaLM
    *   **Legal:** Harvey (A venture valued at approx. $1.5B)
    *   **Coding:** StarCoder, **Retrieval:** Command R+
*   Development methods include From Scratch, Post-training (PEFT), Prompts, RAG, etc.

---

# Why Language Models Now?

**[1] Massive scaling and associated performance improvements**

**[2] Versatility via Prompting**

**[3] Impact on domains beyond language**

---

# Why Language Models Now?

**[1] Massive scaling and associated performance improvements**

[2] Versatility via Prompting

[3] Impact on domains beyond language

---

# Massive Scaling of Transformer-based Language Models

*(Infographic showing the growth of model sizes from 2018 to 2023+. Note the exponential growth in parameters from ELMo/GPT to GPT-4/PaLM.)*

*   Fundamentally, these use the structure called **Transformer**, invented in 2017.
*   Since the advent of GPT-3, US research institutions have developed proprietary large language models.

---

# Why Learn LLMs? 1. Scaling and Emergence

## Scaling Law
*(Graph showing Test Loss decreasing as Compute increases according to a power law)*
*   Performance improves according to a power law regarding **3 variables**.
*   Variables: Compute (C), Dataset Size (D), Number of Parameters (N).

## Emergent Ability
*(Graphs showing accuracy jumping suddenly at specific model scales for tasks like Arithmetic, IPA transliterate, etc.)*
*   **Tasks exist that can only be solved when the model size becomes large.**

---

# "Language Models are Few-Shot Learners", 2020
## GPT-3 Training Data

**GPT-3 Pre-training Token Count**

| Dataset | Quantity (tokens) |
| :--- | :--- |
| Common Crawl (filtered) | 410 billion |
| WebText2 | 19 billion |
| Books1 | 12 billion |
| Books2 | 55 billion |
| Wikipedia | 3 billion |

*   Uses approximately **500 Billion Tokens*** of text.
    *   \* Token is the processing unit of the language AI. In Japanese, roughly 1 character = 1 token.
*   Equivalent to about **5 million books**.
    *   Reference: University of Tokyo Library has approx. 1.3 million books.
    *   National Diet Library has approx. 47 million items.
*   Equivalent to approx. **1.3 trillion links** of link information.

---

# Required Computational Power | Graphic Processing Units

**GPU (H100, A100, V100, etc.)**
*(Image of NVIDIA H100/A100 GPU)*

**Frequently Used GPU Clusters***
*   **ABCI** (AIST)
    *   960 units of A100 GPU (Largest scale in Japan)
*   **Overseas IaaS**
    *   AWS (Amazon), GCP (Google), Azure (Microsoft)

**Estimates:**
*   **GPT-3 equivalent:** A100 $\times$ **1,200 units $\times$ 30 days**
*   **GPT-4 equivalent:** A100 $\times$ **25,000 units $\times$ 100 days**

*\*GPU Cluster: A system that bundles and provides multiple GPUs.*

# Why Language Models Now?

**[1] Massive scaling and associated performance improvements**

**[2] Versatility via Prompting**

[3] Impact on domains beyond language

---

#), "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"*

 Why learn LLMs now? 2. Versatility (Prompting / In-Context Learning)

**Pre-training**
*   **Output:** word probability by analyzing text data
*   **Input:** Language models determine [mask]
*   *Original: Language models determine word probability by analyzing text data*

**Translation (Few-Shot)**
*   ---

# "On the Opportunities and Risks of Foundation Models", 2021

## ■ Supplement | FoundationTranslate English to French: *task description*
*   sea otter => loutre de mer *examples*
*    Model

*   **A term that appeared in a white paper first published on 2021/8/1peppermint => menthe poivrée
*   plush giraffe => girafe peluche
*   cheese =>6**
*   It has also become the name of a research institute at Stanford (Blue frame)
*   **Huge *prompt*

**Translation (Zero-Shot)**
*   Translate English to French: *task description*
 models applicable to diverse tasks**
*   **(Excerpt from Abstract) Paradigm shift**
    *   *“AI is undergoing*   cheese => *prompt*

**Summarization (Zero-Shot)**
*   Starting with "TL; a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3)DR" drastically improves the performance
*   Many other examples

*[7] Cited from Tom Brown et al. (2 that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character”*

*[14] Rishi Bommasani et al. (020), "Language Models are Few-Shot Learners"*

---

# Pre-train, Prompt, Predict

2021) "On the Opportunities and Risks of Foundation Models", modified*

---

# Expertise of GPT-4| Paradigm | Engineering | Task Relation |
| :--- | :--- | :--- |
| **a. Fully ("GPT-4 Technical Report", 2023)

*   **Model announced by OpenAI in 2 Supervised Learning** (Non-Neural Network) | **Features** (e.g. word identity, part-of-speech, sentence length) | CLS / TAG / LM / GEN |
| **b. Fully Supervised023**
    *   (Details are unpublished, though there is leaked information)
*   **Achieved Learning** (Neural Network) | **Architecture** (e.g. convolutional, recurrent, self-attentional high scores in diverse exams such as the Bar Exam and SAT/GRE**
    *   Example: 298/) | CLS / TAG / LM / GEN |
| **c. Pre-train, Fine-tune** | **Objective** (e.g. masked language modeling, next sentence prediction) | CLS / TAG / LM400 (~90th percentile) in the Uniform Bar Exam
    *   Example: 163 / GEN |
| **d. Pre-train, Prompt, Predict** | **Prompt** (e.g. cloze, prefix) | CLS / TAG / LM / GEN |

**(Right Column Flow)**
*   **Traditional:** Train a model for each task (Non-NN)
*   Train a model for each task (NN)
*   Share the model and learn (**Fine-Tuning**)
*   **Current:** **Fix the model** and change instructions (**Prompting**)

*[13] Cited from Pengfei Liu et al. (202/179 (~80th percentile) in GRE (Quantitative)
*   **On the other hand, scores in coding ability etc. were still low** (Currently significantly improved)

*[15] OpenAI 2023 "GPT-4 Technical Report", modified*

---

# "Evaluating gpt-4 and ChatGPT on Japanese medical licensing examinations" 2023

## Igaku-QA | Verification of GPT-4's Expert Knowledge

*(Table showing scores of ChatGPT, GPT-3, GPT-4 on Japanese National Medical Practitioners Qualifying Examination 20181), "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"*-2023)*

*   **Benchmarked language models (GPT-4 and ChatGPT) by constructing a

---

# "On the Opportunities and Risks of Foundation Models", 2021
## ■ Supplement | new dataset of 6 years of Japanese medical licensing exams (Igaku-QA).**
*   **Although there are issues such Foundation Model

**On the Opportunities and Risks of Foundation Models** (Paper Cover)

*   A term that appeared in a as (1) performing worse than the average human examinee and (2) a tendency to select contraindicated options (kinshi-shi), it passed the exam threshold.**

*[16] Jungo Kasai et al. (2023), "Evaluating gpt-4 and ChatGPT on Japanese medical licensing examinations"*

---

# "Flamingo : a Visual Language Model for Few-Shot Learning", 2022, DeepMind

## Example of Large Scale white paper published on **August 16, 2021**.
*   Also serves as the name for a research center at Stanford (CRFM).
*   **Giant models applicable to diverse tasks.**
*   **Paradigm Shift** Model Handling Multimodal Data | Flamingo

*(Images showing inputs of images and text prompts resulting in text completions)*

* (from Abstract):
    *   *“AI is undergoing a paradigm shift with the rise of models (e.   **Integrated a pre-trained Vision Model (NF-Net) and Language Model (Chinchilla, 7g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character”*

*[14] Rishi Bommasani et al. (2021) "On the Opportunities and Risks of Foundation Models" cited0B). Total 80B parameters.**
    *   Connected via pair data (Perceiver Resampler and and partially modified*

---

# GPT-4 Technical Expertise ("GPT-4 Technical Report", 202 Gated Xattn).
*   **Can perform various completions with images and language, such as looking at a photo of a flamingo and returning "A flamingo. They are found in the Caribbean and South America."**

*[21] Jean3)

*(Chart showing GPT-4 performance on various academic benchmarks)*

*   Model announced by **OpenAI in-Baptiste Alayrac et al. (2022), "Flamingo : a Visual Language Model for Few-Shot 2023** (Details are undisclosed, though leaks exist).
*   Achieved good scores on diverse exams such as the **Bar Exam** and **SAT/GRE**.
    *   Example: Uniform Bar Exam 298/ Learning", NeurIPS2022*
*https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model*

---

# Why Language Models400 (**~90th percentile**)
    *   Example: GRE (Quantitative) 163/ Now?

**[1] Massive scaling and associated performance improvements**

**[2] Versatility via Prompting179 (**~80th percentile**)
*   Scores were still low for **coding ability**, etc. (Currently**

**[3] Impact on domains beyond language**

---

# Multimodal Foundation Models

## Image Recognition significantly improved).

*[15] OpenAI 2023 "GPT-4 Technical Report" cited and partially by GPT-4 (and Robot Application)

**(Left Image: GPT-4 visual input example)**
*   User: modified*

---

# "Evaluating gpt-4 and ChatGPT on Japanese medical licensing examinations", 2023
## Igaku-QA | Verification of GPT-4's Technical Expertise

*(Table comparing passing scores of various What is unusual about this image?
*   GPT-4: The unusual thing about this image is that a man is ironing clothes on an ironing board attached to the roof of a moving taxi.

**(Right Image: Video/Figure models from 2018-2023)*

*   Benchmarked language models (GPT-4 and ChatGPT) by constructing a new dataset of **6 years of Japanese Medical Licensing Exams (Igaku-QA)**. AI)**
*   https://www.figure.ai/

*"GPT-4 Technical Report ", 202
*   Although there are issues such as (1) scores being lower than the average human test-taker, and4*

---

# "Do As I Can, Not As I Say: Grounding Language in Robotic Afford (2) a tendency to select **forbidden options (contraindications)**, it **passed the exam border**.

*[ances", 2022

## ■ Utilization of LLMs | Say-Can and Say-Can-Pa16] Jungo Kasai et al. (2023), "Evaluating gpt-4 and ChatGPTLM

*(Diagram showing Instruction Relevance with LLMs combined with Skill Affordances with Value Functions)*

*   ** on Japanese medical licensing examinations" cited*

---

# "Flamingo: a Visual Language Model for Few-Shot Learning", 2022, DeepMind
## Example of a Large Model Handling Multimodal Data | Flamingo

Selects actions by considering the execution possibility (Skill Affordance) of the skill output by the language model.**
    *   Feasibility is learned via TD (Temporal Difference learning).
*   **Improving the language model (using PaLM)*(Diagram showing Input Prompts with images and text, leading to a completion)*
*   Input Prompt: *Images improves performance.**
*   **※ Note that executable skills (low-level policies) are prepared in advance.**

 of animals + "This is a [animal]. They are found in..."* -> Completion: *"a flamingo..."*

*   *[17] Michael Ahn et al. (2022), "Do As I Can, Not As IIntegrated a **Pre-trained Vision Model (NF-Net)** and **Language Model (Chinchilla, 70B Say: Grounding Language in Robotic Affordances"*
```)**. Total 80B parameters.**
    * The connecting part uses paired data (Perceiver Resampler)*

# Research Example at Matsuo Lab
**Generation of Action Sequences (Execution Results)**

**Task:** bring me a noodle from the long table

**(Visual Sequence)**
1.  **Get command**
2.  `go('long table')`
3.  `find_object('noodle', 'long table')`, `grab('noodle')`
4.  `go('start point')`, `find_person('operator', 'start point')`, `hand_over('noodle')`
5.  **Task completed**

**Achievement:** Winner of RoboCup Japan Open 2023, 3rd Place in RoboCup World

---

# "RT-1: Robotics Transformer for Real-World Control at Scale", 2022
**Robot Transformer (RT-1)**

**(Left Image: Architecture Diagram)**
*(a) RT-1 takes images and natural language instructions...*
*(b) RT-1's large-scale, real-world training...*

**Model**
*   Combination of **EfficientNet** and **Transformer**.
*   Generates actions based on instructions.

**Data**
*   13 EDR robots, 17 months, 744 tasks, 130k demos.

**Training**
*   97% success rate for trained actions.

**Generalization**
*   Significant improvement in generalization in various senses (Unknown tasks, unknown sources/environments, etc.).
*   **Long Horizon** tasks are also possible.

*※ Similar research includes Gato, BC-Z, etc.*

*[20] Anthony Brohan et al. (2022), "RT-1: Robotics Transformer for Real-World Control at Scale" cited*

---

# Progress in Image Generation | Sora (Open AI)

**(Left Image)**
*   **Prompt:** Photorealistic closeup video of two pirate ships battling each other as they sail inside a cup of coffee.

**(Right Image)**
*   **Prompt:** A young man at his 20s is sitting on a piece of cloud in the sky, reading a book.

*[67] "Creating video from Text", https://openai.com/sora*
*[68] "Video generation models as world simulators", https://openai.com/research/video-generation-models-as-world-simulators*

---

# Summary so far and Purpose of this Course

## Summary so far
*   **Language Models** model the generation probability of a sequence of words.
    *   Autoregressive Language Models / Neural Language Models / GPT.
*   The **principle is very simple**. Why are they Language Models now?
    1.  Rapid expansion due to **Model, Data, and Compute** (Scaling).
    2.  Ability to do various things with a single model via **Prompting** (Versatility of Language Models).
    3.  Development of Language Models is **influencing other domains**.

## Purpose of this Course
*   To understand the **technical background, principles, and limitations** of LLMs.
*   To be able to perceive them as **technologies to utilize**, not just as hype.

---

# Takeshi Kojima

**Profile**
*   **2023.3:** Completed Ph.D. at The University of Tokyo, Graduate School of Engineering, Department of Technology Management for Innovation.
*   **2023.4 ~:** Project Researcher at The University of Tokyo, Graduate School of Engineering, Department of Technology Management for Innovation.
*   *Previously worked as an IT Engineer.*

**Fields of Interest**
*   Deep Learning, Natural Language Processing, Large Language Models.
*   *(Illustration: "Let's think step by step.")*

**Activities**
*   **Past:** Development of Weblab-10B, Lecturer for the Matsuo Lab LLM Course (previous iteration), Development support at Geniac (Creating standard development code, rule organization, competition evaluation, etc.).
*   **Recent:** Understanding and controlling the operating principles of LLMs, Relationship between data and capability, AI Safety, Lightweighting/Distillation, etc.

---

# Table of Contents

*   **Overview of LLMs**
*   **Overview of each session**
*   **The environment surrounding LLMs in Japan**

---

# Structuring the Course

**[First Half Sessions]**
Understand how to use and create LLMs, and acquire basic techniques (Model, Training, Data, etc.).

**[Second Half Sessions]**
Deep dive into topics applying the techniques learned in the first half (Domain specificity, Robotics and LLMs, etc.).

---

# Overall Structure of LLM Course 2024 (1)

*   **Session 1:** Overview of Language Models **<-- You are here**
*   **Session 2:** Prompting and RAG
*   **Session 3:** Pre-training
*   **Session 4:** Scaling Law
*   **Session 5:** Supervised Fine-Tuning
*   **Session 6:** Semiconductor Ecosystem supporting LLM development
*   **Session 7:** RLHF & Alignment
*   **Session 8:** Advanced Pre-training
    *   *(Blue Box: First Half - Basics)*

*   **Session 9:** Safety
*   **Session 10:** Analysis and Theory of LLMs
*   **Session 11:** Application of LLM (Domain Specific LLM)
*   **Session 12:** Application of LLM (LLM for Control)
*   **Final Assignment:** Student competition to solve LLM-related tasks.
    *   *(Green Box: Second Half - Application)*

---

# Overall Structure of LLM Course 2024 (2)

*   **Session 1: Overview of Language Models**
*   **Session 2: Prompting and RAG**
*   **Session 3: Pre-training**
*   **Session 4: Scaling Law**
*   **Session 5: Supervised Fine-Tuning**
*   **Session 6:** Semiconductor Ecosystem supporting LLM development
*   **Session 7: RLHF & Alignment**
*   **Session 8: Advanced Pre-training**
*   **Session 9:** Safety
*   **Session 10:** Analysis and Theory of LLMs
*   **Session 11:** Application of LLM (Domain Specific LLM)
*   **Session 12:** Application of LLM (LLM for Control)
*   **Final Assignment:** Student competition.

*(Red Callout Box)*
**Sessions 1-5 and 7-8 overlap with the 2023 lectures, but have been updated!**
